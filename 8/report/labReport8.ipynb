{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d590df2d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ESSE 2220 – Lab 8 Report\n",
    "## Feature Matching in Satellite Images (BFMatcher & FLANN)\n",
    "\n",
    "**Course:** ESSE 2220  \n",
    "**Lab Title:** Feature Matching in Satellite Images (BFMatcher & FLANN)  \n",
    "**Date Performed:** 2025-11-14  \n",
    "**Date Submitted:** 2025-11-14\n",
    "\n",
    "---\n",
    "\n",
    "## Names & Group Info\n",
    "**Group Number:** GROUP 5  \n",
    "**Members:** Yathharthha Kaushal · Owen Oliver\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Dataset Pair\n",
    "### 1.1 The downloaded satellite image pair used for this lab is pair 8 with the filenames: \n",
    "- socal-fire_00000325_pre_disaster.png\n",
    "- socal-fire_00000325_post_disaster.png\n",
    "### 1.2 Original Images\n",
    "**Before Disaster:**  \n",
    "\n",
    "![Before Disaster](https://raw.githubusercontent.com/YK12321/ESSE2220-Labs/main/8/code/socal-fire_00000325_pre_disaster.png)  \n",
    "\n",
    "**After Disaster:**  \n",
    "\n",
    "![After Disaster](https://raw.githubusercontent.com/YK12321/ESSE2220-Labs/main/8/code/socal-fire_00000325_post_disaster.png)\n",
    "\n",
    "### 1.3 Scene Description\n",
    "> The scene shows a semi-urban area with a mix of residential buildings to the right, and hilly terrain with vegetation to the left, the road seperating the two areas is surrounded by dirt, with some dirt trails branching off.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Summary Table of All Methods\n",
    "|Matcher | Detector | Raw Matches | Distance Filter | Ratio Test | Symmetry | Observation|\n",
    "|-------|----------|-------------|-----------------|------------|----------|------------|\n",
    "| BF | SIFT | 10664 | 1701 | 662 | 3471 | Dense but noisy until filtered |\n",
    "| BF | ORB | 500 | 500 | 21 | 193 | Few matches yet mostly crisp around structures |\n",
    "| FLANN | SIFT | 10664 | 1612 | 720 | -- | Slightly noisier because ANN search returns near-duplicates |\n",
    "| FLANN | ORB | 489 | 489 | 37 | -- | Sparse but consistent thanks to binary descriptors |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Questions About Your Table\n",
    "### 3.1 Which combination produced the largest number of raw matches? Why?\n",
    " > BF + SIFT (and equivalently FLANN + SIFT) produced the most raw matches at 10 664 because SIFT detects many high-quality scale-invariant keypoints in this textured fire scene, giving the matcher far more descriptors to pair than ORB.\n",
    "\n",
    "### 3.2 Which filtering method removed the most mismatches?\n",
    " > The Lowe ratio test on the SIFT descriptors removed the most mismatches (dropping BF matches from 10 664 to 662 and FLANN matches to 720) because comparing the top two candidates suppresses ambiguous pairings that share similar distances.\n",
    "\n",
    "### 3.3 Which method (BF or FLANN) gave more stable results between your two satellite images?\n",
    " > BFMatcher was more stable: both the raw and filtered BF counts were consistent between detectors, and the symmetry cross-check ensured each match was mutual, so small illumination or viewpoint shifts between the pre/post images did not change the retained matches much.\n",
    "\n",
    "### 3.4 Which detector (SIFT or ORB) worked better for your pair?\n",
    " > SIFT worked better—thousands of its matches survived the filters while ORB retained only a few dozen after quality checks, indicating that scale and rotation invariance from SIFT was necessary for this scene’s fine structural changes.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Feature Extraction (SIFT and ORB) – Timing + Descriptors\n",
    "### 4.1 Extraction Time for Each Image and Detector\n",
    "**SIFT**\n",
    "| Image | Keypoints | Descriptor Shape | Dtype | Extraction Time (s) |\n",
    "|-------|-----------|------------------|-------|---------------------|\n",
    "| PRE  | 10,664 | (10664, 128) | float32 | 0.3962 |\n",
    "| POST | 7,727 | (7727, 128) | float32 | 0.5271 |\n",
    "\n",
    "**ORB**\n",
    "| Image | Keypoints | Descriptor Shape | Dtype | Extraction Time (s) |\n",
    "|-------|-----------|------------------|-------|---------------------|\n",
    "| PRE  | 500 | (500, 32) | uint8 | 0.0425 |\n",
    "| POST | 500 | (500, 32) | uint8 | 0.0379 |\n",
    "\n",
    "### 4.2 Why ORB is Typically Faster than SIFT\n",
    "> ORB relies on FAST keypoint detection plus BRIEF-style binary descriptors, so it mostly performs intensity comparisons and bit operations at a single scale pyramid. SIFT, in contrast, builds multi-scale Difference-of-Gaussian pyramids, fits sub-pixel extrema, and forms 128-D histogram descriptors with floating-point gradients. The additional octave construction and floating-point math make SIFT far heavier per keypoint, which is why my timings show ORB finishing in ~0.04 s while SIFT takes roughly ten times longer on the same images.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Filtering Methods – BFMatcher\n",
    "### 5.1 Distance Filter\n",
    "> **Distance Threshold:** Removes obviously bad matches where the descriptor distance is large (e.g., above 225 for ORB), so it keeps only the closest-looking correspondences in descriptor space.  \n",
    " > **Lowe’s Ratio Test:** For each keypoint it compares the best and second-best neighbours; if the best distance is not at least ~30% smaller, that match is likely ambiguous and gets discarded.  \n",
    " > **Symmetry / Cross-Check:** Forces mutual agreement by keeping only pairs where feature A’s best match is feature B and vice versa, which drops one-sided or repetitive texture matches that tend to be false positives.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Filtering Methods – FLANN\n",
    "### 6.1 Why FLANN Might Produce Slightly Different Results Each Run\n",
    "> FLANN performs approximate nearest-neighbour searches that use randomized trees (KD-trees or LSH tables) and sample only a subset of leaves per query. The random seeding and the probabilistic search order mean two runs can explore different branches, so the exact neighbour returned—and therefore the match counts—can drift slightly between executions.\n",
    "\n",
    "### 6.2 What Type of Mismatches Each Filter Removes\n",
    "> **Distance Threshold:** Applies the same hard cutoff to FLANN’s approximate matches, trimming very weak pairings that still slipped through the ANN search.  \n",
    " > **Lowe’s Ratio Test:** Works the same as with BF but guards against approximate neighbours by ensuring the top candidate is significantly better than the runner-up, which rejects many near-duplicate keypoints in vegetation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec93ef",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "## A1. All 14 Required Output Images\n",
    "<button onclick=\"window.location.href='https://github.com/YK12321/ESSE2220-Labs/tree/main/8/code/output'\">View Output Images</button>  \n",
    "Or if this button isn't working, then just type this url in your browser: https://github.com/YK12321/ESSE2220-Labs/tree/main/8/code/output\n",
    "\n",
    "## A2. Complete Python Code\n",
    "```python\n",
    "\"\"\"\n",
    "Lab 8: Feature Matching (BFMatcher + FLANN + Filtering)\n",
    "-------------------------------------------------------\n",
    "Instructions:\n",
    "1. Choose ONE detector at a time (SIFT or ORB).\n",
    "2. Extract features and measure extraction time.\n",
    "3. Run BFMatcher (normal and crossCheck).\n",
    "4. Run FLANN with\n",
    "5. Apply THREE filters:\n",
    "       - Distance Threshold\n",
    "       - Lowe's Ratio Test\n",
    "       - Symmetry Check (BFMatcher crossCheck=True)\n",
    "6. Visualize all results\n",
    "7. Save all required images.\n",
    "8. Repeat with the second detector.\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 1: Load images\n",
    "# ---------------------------------------------------------\n",
    "img1 = cv2.imread(\"socal-fire_00000325_pre_disaster.png\", cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread(\"socal-fire_00000325_post_disaster.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 2: Choose ONE detector (uncomment one)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# ---- SIFT ----\n",
    "# detector = cv2.SIFT_create()\n",
    "\n",
    "# ---- ORB ----\n",
    "detector = cv2.ORB_create()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 3: Detect + compute (with timing)\n",
    "# ---------------------------------------------------------\n",
    "t0 = time.time()\n",
    "kp1, des1 = detector.detectAndCompute(img1, None)\n",
    "t1 = time.time()\n",
    "kp2, des2 = detector.detectAndCompute(img2, None)\n",
    "t2 = time.time()\n",
    "\n",
    "print(\"Keypoints img1:\", len(kp1))\n",
    "print(\"Keypoints img2:\", len(kp2))\n",
    "\n",
    "print(\"Descriptor shapes:\", des1.shape, des2.shape)\n",
    "\n",
    "print(\"Extraction time (img1):\", t1 - t0)\n",
    "print(\"Extraction time (img2):\", t2 - t1)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 4: Create matchers (BF + crossCheck + FLANN)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# BF normal matcher\n",
    "# (For SIFT → NORM_L2, For ORB → NORM_HAMMING)\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING)\n",
    "\n",
    "# BF symmetry check matcher\n",
    "bf_cross = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "# FLANN matcher\n",
    "# algorithm = type of search structure (1 = KDTree, 6 = LSH)\n",
    "# trees = number of trees (more trees → slower but more accurate)\n",
    "# checks = how many leaf nodes to search (higher = better accuracy)\n",
    "flann = cv2.FlannBasedMatcher(\n",
    "    indexParams = dict(algorithm=6, trees=5),\n",
    "    searchParams = dict(checks=50)\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 5: Perform matching (raw outputs)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Best match for each descriptor\n",
    "matches_bf = bf.match(des1, des2)\n",
    "\n",
    "# Two best matches (m,n) for each descriptor\n",
    "knn_bf = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "# FLANN single best match (approximate)\n",
    "matches_flann = flann.match(des1, des2)\n",
    "\n",
    "# FLANN two best matches (approximate)\n",
    "knn_flann = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 6: Filtering methods\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Distance Threshold\n",
    "# Typical range:\n",
    "#   SIFT: 150–300\n",
    "#   ORB:  20–60\n",
    "def distance_filter(matches, threshold):\n",
    "    return [m for m in matches if m.distance < threshold]\n",
    "\n",
    "\n",
    "# Lowe’s Ratio Test (m.distance < ratio * n.distance)\n",
    "# Typical range: 0.6–0.8\n",
    "def ratio_test(knn_matches, ratio=0.75):\n",
    "    good = []\n",
    "    # knn_matches is a list-of-lists: each element contains up to k matches.\n",
    "    # E.g., when k=2, knnMatch normally returns pairs like [m, n].\n",
    "    # But sometimes FLANN/BFMatcher will return fewer than k matches for a\n",
    "    # descriptor (for example when a second-neighbour couldn't be found).\n",
    "    #\n",
    "    # The old version did `for m, n in knn_matches:` which assumes every list\n",
    "    # element has at least two values; that causes a ValueError when a\n",
    "    # singleton list appears. To prevent that, we check the length and skip\n",
    "    # entries that don't have two matches.\n",
    "    for m_n in knn_matches:\n",
    "        # make sure we have at least two neighbours; skip if not\n",
    "        if not m_n or len(m_n) < 2:\n",
    "            # Skipping entries with only one or zero matches avoids unpacking\n",
    "            # errors and is consistent with common implementations of\n",
    "            # Lowe's ratio test (which compares the best and second-best).\n",
    "            continue\n",
    "        # Unpack first and second best matches and apply Lowe's ratio test\n",
    "        m, n = m_n[0], m_n[1]\n",
    "        if m.distance < ratio * n.distance:\n",
    "            good.append(m)\n",
    "    return good\n",
    "\n",
    "\n",
    "# Symmetry Check: already done using BFMatcher crossCheck=True\n",
    "matches_sym = bf_cross.match(des1, des2)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 7: Apply ONE method at a time\n",
    "# ---------------------------------------------------------\n",
    "# Students should comment/uncomment ONE line:\n",
    "\n",
    "# chosen_matches = matches_bf                    # Raw BF\n",
    "# chosen_matches = matches_flann                # Raw FLANN\n",
    "# chosen_matches = distance_filter(matches_bf, threshold=225)\n",
    "# chosen_matches = ratio_test(knn_bf, ratio=0.7)\n",
    "# chosen_matches = matches_sym                  # Symmetry check\n",
    "# chosen_matches = distance_filter(matches_flann, threshold=225)\n",
    "chosen_matches = ratio_test(knn_flann, ratio=0.7)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Step 8: Visualization (ONE LINE ONLY)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "img_out = cv2.drawMatches(img1, kp1, img2, kp2, chosen_matches, None)\n",
    "cv2.imwrite(\"flann_ratio.jpg\", img_out)\n",
    "print(\"Matches: \", len(chosen_matches))\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# REQUIRED OUTPUT IMAGES\n",
    "# ---------------------------------------------------------\n",
    "\"\"\"\n",
    "For EACH detector (SIFT, ORB):\n",
    "\n",
    "1. raw_bf.jpg               (matches_bf)\n",
    "2. raw_flann.jpg            (matches_flann)\n",
    "3. bf_distance.jpg          (distance filter on BF)\n",
    "4. bf_ratio.jpg             (ratio test on BF)\n",
    "5. bf_symmetry.jpg          (crossCheck output)\n",
    "6. flann_distance.jpg       (distance filter on FLANN)\n",
    "7. flann_ratio.jpg          (ratio test on FLANN)\n",
    "\n",
    "Total: 7 images per detector → 14 images in the lab.\n",
    "\"\"\"\n",
    "```\n",
    "## Performance analysis helper\n",
    "```python\n",
    "\"\"\"Lab 8 – Section 4 helper script.\n",
    "\n",
    "This streamlined version only gathers the numbers needed for\n",
    "Section 4 of the report (feature extraction time + descriptor info\n",
    "for both SIFT and ORB). No matchers or images are produced here.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "IMAGE_NAMES = {\n",
    "    \"pre\": \"socal-fire_00000325_pre_disaster.png\",\n",
    "    \"post\": \"socal-fire_00000325_post_disaster.png\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_images():\n",
    "    \"\"\"Load the grayscale satellite pair and fail loudly if missing.\"\"\"\n",
    "    base_dir = Path(__file__).resolve().parent\n",
    "    images = {}\n",
    "    for label, name in IMAGE_NAMES.items():\n",
    "        path = base_dir / name\n",
    "        img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Could not open {path}\")\n",
    "        images[label] = img\n",
    "    return images\n",
    "\n",
    "\n",
    "def time_extraction(detector, image, warmup=True):\n",
    "    \"\"\"Detect keypoints/descriptors and report duration.\"\"\"\n",
    "    if warmup:\n",
    "        # First call primes OpenCV's internal buffers so the timed run\n",
    "        # measures steady-state performance instead of one-time setup cost.\n",
    "        detector.detectAndCompute(image, None)\n",
    "    start = time.perf_counter()\n",
    "    keypoints, descriptors = detector.detectAndCompute(image, None)\n",
    "    duration = time.perf_counter() - start\n",
    "    return keypoints, descriptors, duration\n",
    "\n",
    "\n",
    "def describe_descriptors(descriptors):\n",
    "    if descriptors is None:\n",
    "        return \"None\", \"None\"\n",
    "    return str(descriptors.shape), str(descriptors.dtype)\n",
    "\n",
    "\n",
    "def main():\n",
    "    images = load_images()\n",
    "    detector_factories = {\n",
    "        \"SIFT\": cv2.SIFT_create,\n",
    "        \"ORB\": cv2.ORB_create,\n",
    "    }\n",
    "\n",
    "    for detector_name, create_detector in detector_factories.items():\n",
    "        print(f\"=== {detector_name} ===\")\n",
    "        for label, image in images.items():\n",
    "            detector = create_detector()\n",
    "            keypoints, descriptors, duration = time_extraction(detector, image)\n",
    "            shape, dtype = describe_descriptors(descriptors)\n",
    "            print(\n",
    "                f\"{label.upper()} | keypoints: {len(keypoints):5d} | \"\n",
    "                f\"descriptor shape: {shape} | dtype: {dtype}\"\n",
    "            )\n",
    "            print(f\"Extraction time: {duration:.4f} s\")\n",
    "        print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
