{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952276e2",
   "metadata": {},
   "source": [
    "# ESSE 2220 – Lab 7 Report\n",
    "## Detecting Features in Space, SIFT and ORB in Action\n",
    "\n",
    "**Course:** ESSE 2220  \n",
    "**Lab Title:** Detecting Features in Space, SIFT and ORB in Action\n",
    "**Date Performed:** 2025-11-07  \n",
    "**Date Submitted:** 2025-11-07\n",
    "\n",
    "---\n",
    "\n",
    "## Names & Group Info\n",
    "**Group Number:** GROUP 5  \n",
    "**Members:** Yathharthha Kaushal · Owen Oliver\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Image Loading and Channels:\n",
    "\n",
    "### 1.1 Code output for IMGREAD_GRAYSCALE:\n",
    "```python\n",
    "img = cv2.imread('space.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "```\n",
    "\n",
    "### 1.2 Code output for IMGREAD_COLOR:\n",
    "```python\n",
    "img = cv2.imread('space.jpg', cv2.IMREAD_COLOR)\n",
    "```\n",
    "\n",
    "### 1.3 Printed shapes:\n",
    ">Grayscale image shape: (1056, 1280)\n",
    ">Color image shape: (1056, 1280, 3)\n",
    "\n",
    "### 1.4 What changed and shape differences:\n",
    "> The grayscale image has a shape of (1056, 1280), indicating it has only two dimensions (height and width). The color image has a shape of (1056, 1280, 3), indicating it has three dimensions - the first two can be implied as the height and width, and the third dimension represents the number of the color channels (Red, Green, Blue).\n",
    "\n",
    "## 2. SIFT Detection and Visualization:\n",
    "\n",
    "### 2.1 Sift Keypoint image:\n",
    "[![SIFT Keypoints](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/sift_keypoints.jpg)](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/sift_keypoints.jpg)\n",
    "\n",
    "### 2.2 Color chosen for keypoints\n",
    "\n",
    "#### 2.2.1 Color chosen:\n",
    "<div style=\"width:100px;height:100px;background:rgb(255,0,255);\"></div>\n",
    "\n",
    "> Bright magenta (RGB: 255, 0, 255) was chosen because the bright it rarely occurs in raw starfield imagery, and is easily distinguishable from the dark background, and is not confused with common blue/red nebulas.\n",
    "#### 2.2.2 Explain how I designed it:\n",
    "> I looked for a color that would: stand out visibly against the dark background of space and not be confused with colors commonly found in starts, nebulae, other space objects. Picking the color was rather trivial, as it was just full red and full blue, with no green.\n",
    "\n",
    "### 2.3 Number of keypoints detected and descriptor shape:\n",
    "> Number of keypoints (SIFT): 1391  \n",
    "> Descriptor shape (SIFT): (1391, 128)  \n",
    "\n",
    "### 2.4 Explanation of keypoints:\n",
    "> A keypoint is a certain feature in the image that is considered interesting or important for analysis. In the context of SIFT (Scale-Invariant Feature Transform), keypoints are specific locations in the image that are invariant to scale, rotation, and illumination changes. These keypoints are typically corners, edges, or blobs in the image that can be reliably detected and used for tasks such as image matching, object recognition, and 3D reconstruction.\n",
    "\n",
    "## 3. ORB Detection and Visualization:\n",
    "\n",
    "### 3.1 ORB Keypoint image:\n",
    "[![ORB Keypoints](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/orb_keypoints.jpg)](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/orb_keypoints.jpg)\n",
    "\n",
    "### 3.2 Color chosen for keypoints\n",
    "> The reasoning is the same as in 2.2\n",
    "\n",
    "### 3.3 Number of keypoints detected and descriptor shape:\n",
    "> Number of keypoints (ORB): 500\n",
    "> Descriptor shape (ORB): (500, 32)\n",
    "\n",
    "### 3.4 What values did I use for nfeatures.\n",
    "> For the first iteration, I set the value as 500, and the number of detection points was relatively high\n",
    "> For the second iteration, I set the value as 5, and the number of detection points was much smaller.\n",
    "\n",
    "## 4. Strongest Keypoint\n",
    "### 4.1 Top 50 Keypoint images:\n",
    "#### 4.1.1 SIFT Top 50 Keypoints:\n",
    "[![Top 50 SIFT Keypoints](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/sift_top50.jpg)](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/sift_top50.jpg)\n",
    "#### 4.1.2 ORB Top 50 Keypoints:\n",
    "[![Top 50 ORB Keypoints](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/orb_top50.jpg)](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/orb_top50.jpg)\n",
    "\n",
    "### 4.2 What does the response for the keypoint represent?\n",
    "> The response for a keypoint represents the strength of a keypoint (i.e. how unique the feature is), the higher the value, the easier it is to recognize that keypoint in different images or under different conditions.\n",
    "\n",
    "### 4.3 Why might strong keypoints be useful in later matching steps?\n",
    "> Strong keypoints will be useful in matching steps because they are more likely to be easily identifiable/distinguisable between multiple images, which would make them more reliable for matching features across images.\n",
    "\n",
    "## 5. Descriptor Values:\n",
    "### 5.1 First few descriptor values for SIFT and ORB:\n",
    "> First two SIFT descriptor values:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "367a4936",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[ 11.   2.   0.   0.   0.   1.   2.  10.  25.  13.   1.   0.   0.   0.\n",
    "   17.  32.  12.   3.   0.   0.   0.  34.  79.  28.   4.   1.   0.   0.\n",
    "    0.  55.  29.   6.   8.   3.   2.   2.   2.   1.   1.   8.  84.  47.\n",
    "   11.   2.   1.   0.   2.  12. 150.  18.   0.   0.   3.  55.  84. 137.\n",
    "   12.   1.   0.   0.  79. 150.  66.  24.   2.   1.   0.   2.   6.   3.\n",
    "    2.   3.  93.  20.   1.   1.   2.   1.   4.  22. 150. 150.  11.   5.\n",
    "   13.  13.   8.  35.  42.  51.  22.  52. 150. 107.   9.   9.   0.   0.\n",
    "    0.   0.   0.   0.   0.   0.  21.  23.   0.   0.   0.   0.   0.   0.\n",
    "   80. 150.  25.   4.   1.   0.   0.   0.   7.  68.  61. 112.  58.   1.\n",
    "    0.   0.]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74e718ce",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[ 14.   1.   1.   0.   0.   5.   7.  13. 139.   0.   0.   0.   3.   2.\n",
    "    3. 139.  50.   0.   0.  27. 139.  16.   5.  65.   0.   0.   0.  35.\n",
    "  112.  13.   0.   0.  61.   9.   1.   0.   0.   0.   0.   8. 139.  30.\n",
    "    5.  11.  20.   1.   1.  82.  52.   8.   5. 131. 139.   4.   1.  23.\n",
    "    0.   0.   3.  62.  97.   0.   0.   0.  42.  23.   1.   0.   0.   0.\n",
    "    0.   0. 138. 139.  92.  30.   6.   0.   0.   2.  11.  29.  87. 139.\n",
    "  110.   0.   0.   1.   0.   0.   1.  32.  37.   7.   5.   1.   2.  10.\n",
    "    2.   0.   0.   0.   0.   0.   2.  29.  48.   4.   0.   0.   0.   1.\n",
    "    4.   5.  39.  24.   1.   0.   0.   3.   5.   2.   1.   3.   1.   6.\n",
    "    5.   4.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d39a2b",
   "metadata": {},
   "source": [
    ">First two ORB descriptor values:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a633d25b",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[244  96 145 100   8  41  65   8 162 178 160  24 215 211  73  26  17 220\n",
    "  104  24  10  71 218   2 217 132   5  60 224  49   2  32]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7eaf8d7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[252 108 148 117 168  42  69 187 242 186 174  88 211  49 193  27   1  70\n",
    "  108  58 121  97 203  32 232 226 117  52 232 241 194  32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b8033a",
   "metadata": {},
   "source": [
    "### 5.2 Explanation of descriptor vectors:\n",
    "> A descriptor vector represents the local image patch around a keypoint in a compact numerical format. It captures the essential features of the image patch, such as texture, intensity gradients, and orientation. Descriptor vectors are used to compare and match keypoints between different images.\n",
    "\n",
    "### 5.3 Describe one difference between SIFT and ORB descriptors:\n",
    "> One difference between SIFT and ORB descriptors is that SIFT descriptors are 128-dimensional floating-point vectors, while ORB descriptors are 32-dimensional binary vectors. This means that SIFT descriptors provide a more detailed representation of the local image patch, while ORB descriptors are more compact and faster for matching (more suitable for real-time applications).\n",
    "\n",
    "## 6. Descriptor Visualization:\n",
    "### 6.1 SIFT Descriptor Plot:\n",
    "[![SIFT Descriptor Plot](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/sift_descriptor_plot.jpg)](https://raw.githubusercontent.com/YK12321/ESSE2220-labs/main/7/code/sift_descriptor_plot.jpg)\n",
    "\n",
    "### 6.2 Matplotlib functions used for plotting:\n",
    "`figure()` - This was used to create a new blank figure for the plot.  \n",
    "`bar()` - This was used for defining that the plot type is a bar chart, and provide the raw data to be plotted.  \n",
    "`title()`, `xlabel()`, `ylabel()` - Pretty self-explanatory, these functions were used to set the title and axis labels for the plot.  \n",
    "`savefig()` - Saved the generated plot as an image file.  \n",
    "`close()` - Prevents memory leaks by closing the plot after saving.\n",
    "\n",
    "### 6.3 Explanation of the plot:\n",
    "> The plot represents the values of the SIFT descriptor for the first keypoint. The x-acis represents the index of each element in the descriptor vector (starting from 1 and ending at 128), while the y-axis represents the corresponding value of each element in the descriptor vector. The height of each bar indicates the direction of the feature in that dimension, with taller bars indicating stronger features.\n",
    "\n",
    "\n",
    "## 7. Program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c768104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lab 7: Feature Detection in Space Images (SIFT & ORB)\n",
    "-----------------------------------------------------\n",
    "Instructions:\n",
    "1. Fill in the missing parts using the OpenCV documentation.\n",
    "2. You must look up how to use the functions marked with TODO.\n",
    "3. Use the official docs: https://docs.opencv.org/\n",
    "   or Google search like: cv2.SIFT_create site:docs.opencv.org\n",
    "4. Save all your output images using cv2.imwrite().\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Step 1: Read the image ---\n",
    "img = cv2.imread('space.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# TODO: Try IMREAD_COLOR instead.\n",
    "img_color = cv2.imread('space.jpg', cv2.IMREAD_COLOR)\n",
    "# Then print img.shape for both cases and describe what changes.\n",
    "print(\"Grayscale image shape:\", img.shape)\n",
    "print(\"Color image shape:\", img_color.shape)\n",
    "# What does the shape tell you about the number of channels (grayscale vs color)?\n",
    "# Example: print(img.shape)\n",
    "\"\"\"Grayscale image has two dimensions (height, width), whereas\n",
    "the Color image has three dimensions (height, width, 3(we can hypothesize number of color channels (i.e. RGB)))\"\"\"\n",
    "\n",
    "\n",
    "# --- Step 2: Create SIFT detector ---\n",
    "# TODO: Find how to create a SIFT detector object in the docs.\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# --- Step 3: Detect keypoints and descriptors ---\n",
    "# TODO: Look up how to detect and compute both at once.\n",
    "keypoints_sift, descriptors_sift = sift.detectAndCompute(img, None)\n",
    "\n",
    "# --- Step 4: Print some info ---\n",
    "print(\"Number of keypoints (SIFT):\", len(keypoints_sift))\n",
    "print(\"Descriptor shape (SIFT):\", descriptors_sift.shape)\n",
    "\n",
    "# --- Step 5: Draw SIFT keypoints ---\n",
    "# TODO: Look up drawKeypoints function in docs.\n",
    "# Search what flags can be used (hint: something about “rich keypoints”).\n",
    "# Design your own color for SIFT visualization using an RGB tuple, e.g., (R, G, B).\n",
    "# Write in your report what color it represents and how you designed it.\n",
    "img_sift = cv2.drawKeypoints(img, keypoints_sift, None, (255, 0, 255))\n",
    "\"\"\"This color was picked because the bright magenta color rarely occurs in raw\n",
    "starfield imagery, and is easily distinguishable from the dark background, and is \n",
    "not cofused with common blue/red nebulas\"\"\"\n",
    "\n",
    "# Save the output image.\n",
    "cv2.imwrite('sift_keypoints.jpg', img_sift)\n",
    "\n",
    "# --- Step 6: Create ORB detector ---\n",
    "# TODO: Find ORB_create and its parameters (e.g., nfeatures).\n",
    "# Hint: A common starting value is around 500, but try different values (like 100, 1000)\n",
    "# and describe how changing this number affects the number of detected keypoints.\n",
    "orb = cv2.ORB_create(nfeatures=500)\n",
    "\n",
    "# --- Step 7: Detect and compute with ORB ---\n",
    "keypoints_orb, descriptors_orb = orb.detectAndCompute(img, None)\n",
    "\n",
    "print(\"Number of keypoints (ORB):\", len(keypoints_orb))\n",
    "print(\"Descriptor shape (ORB):\", descriptors_orb.shape)\n",
    "\n",
    "# --- Step 8: Draw ORB keypoints ---\n",
    "# TODO: Draw keypoints using a different RGB color than SIFT.\n",
    "# You must also add appropriate flags for drawing.\n",
    "# Explain what color you chose and why.\n",
    "img_orb = cv2.drawKeypoints(img, keypoints_orb, None, (255, 0, 255))\n",
    "cv2.imwrite('orb_keypoints.jpg', img_orb)\n",
    "\n",
    "# --- Step 9: Top 50 strongest keypoints ---\n",
    "# TODO: Use the Python function \"sorted\" to sort keypoints by their response value.\n",
    "# You must Google what sorted() does and how to use the \"key\" parameter.\n",
    "# \"response\" measures how strong or distinctive a detected feature is — higher = stronger feature.\n",
    "# Then select the top 50 keypoints for both SIFT and ORB and visualize them.\n",
    "# Save the resulting images as 'sift_top50.jpg' and 'orb_top50.jpg'.\n",
    "def getResponse(kp):\n",
    "    return kp.response\n",
    "\n",
    "sorted_sift = sorted(keypoints_sift, key=getResponse, reverse=True)\n",
    "sorted_orb = sorted(keypoints_orb, key=getResponse, reverse=True)\n",
    "\n",
    "sorted_sift = sorted_sift[:50]\n",
    "sorted_orb = sorted_orb[:50]\n",
    "\n",
    "#for i, (kps, kpo) in enumerate(zip(sorted_sift, sorted_orb)):\n",
    "#    print(i, kps.response, kpo.response)\n",
    "\n",
    "\n",
    "print(\"Top 50 SIFT keypoints responses:\", len(sorted_sift))\n",
    "\n",
    "cv2.imwrite(\"sift_top50.jpg\", cv2.drawKeypoints(img, sorted_sift, None, (255, 0, 255)))\n",
    "cv2.imwrite(\"orb_top50.jpg\", cv2.drawKeypoints(img, sorted_orb, None, (255, 0, 255)))\n",
    "\n",
    "# --- Step 10: Print and explain descriptors ---\n",
    "# Print the first few descriptor values for each method.\n",
    "# Example: print(descriptors_sift[:2]) and print(descriptors_orb[:2])\n",
    "# In your report, explain what these descriptor values represent and how they differ.\n",
    "print(descriptors_sift[:2])\n",
    "print(descriptors_orb[:2])\n",
    "\n",
    "# --- Step 11: Visualize one descriptor vector ---\n",
    "# TODO: Use matplotlib to create a bar chart of one descriptor vector.\n",
    "# Hint: Search “matplotlib bar chart site:matplotlib.org”\n",
    "# Also check how to install matplotlib if you don’t have it installed.\n",
    "# (Hint: pip install matplotlib)\n",
    "# Save the figure as 'sift_descriptor_plot.jpg'.\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(list(range(1, len(descriptors_sift[0]) + 1)), descriptors_sift[0])\n",
    "plt.title('SIFT Descriptor Vector')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.savefig('sift_descriptor_plot.jpg')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
